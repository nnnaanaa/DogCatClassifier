{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04087662-a99c-460a-b441-b60d896591f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2240, 64, 64, 3)\n",
      "X_test shape: (50, 64, 64, 3)\n",
      "y_train shape: (2240,)\n",
      "y_test shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "classes = [\"dog\", \"cat\"]\n",
    "num_classes = len(classes)\n",
    "image_size = 64\n",
    "num_testdata = 25\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for index, classlabel in enumerate(classes):\n",
    "    photos_dir = \"./\" + classlabel\n",
    "    files = glob.glob(photos_dir + \"/*.jpg\")\n",
    "    for i, file in enumerate(files):\n",
    "        image = Image.open(file)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = image.resize((image_size, image_size))\n",
    "        data = np.asarray(image)\n",
    "        if i < num_testdata:\n",
    "            X_test.append(data)\n",
    "            y_test.append(index)\n",
    "        else:\n",
    "            for angle in range(-20, 20, 5):\n",
    "                img_r = image.rotate(angle)\n",
    "                data = np.asarray(img_r)\n",
    "                X_train.append(data)\n",
    "                y_train.append(index)\n",
    "\n",
    "                img_trains = img_r.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                data = np.asarray(img_trains)\n",
    "                X_train.append(data)\n",
    "                y_train.append(index)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# xy = (X_train, X_test, y_train, y_test)\n",
    "np.savez(\"./dog_cat.npz\", X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "181a3c6d-8820-448e-8d6f-ed05116a971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "classes = [\"dog\", \"cat\"]\n",
    "num_classes = len(classes)\n",
    "image_size = 64\n",
    "\n",
    " #データを読み込む関数 \n",
    "\n",
    "def load_data():\n",
    "    loaded_arrays = np.load(\"./dog_cat.npz\", allow_pickle = True)\n",
    "    X_train = loaded_arrays[\"X_train\"]\n",
    "    X_test = loaded_arrays[\"X_test\"]\n",
    "    y_train = loaded_arrays[\"y_train\"]\n",
    "    y_test = loaded_arrays[\"y_test\"]\n",
    "\n",
    "    X_train = X_train.astype(\"float\") / 255\n",
    "    X_test = X_test.astype(\"float\") / 255\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53edf0c7-6e3c-4545-9db1-d28c980aa446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.5561 - loss: 0.6912\n",
      "Epoch 2/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - accuracy: 0.5999 - loss: 0.6614\n",
      "Epoch 3/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.6417 - loss: 0.6355\n",
      "Epoch 4/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.6968 - loss: 0.5889\n",
      "Epoch 5/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.6937 - loss: 0.5668\n",
      "Epoch 6/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.7145 - loss: 0.5450\n",
      "Epoch 7/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.7757 - loss: 0.4894\n",
      "Epoch 8/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.7937 - loss: 0.4552\n",
      "Epoch 9/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.8060 - loss: 0.4201\n",
      "Epoch 10/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.8359 - loss: 0.3824\n",
      "Epoch 11/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.8682 - loss: 0.3411\n",
      "Epoch 12/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.9004 - loss: 0.2784\n",
      "Epoch 13/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.9018 - loss: 0.2678\n",
      "Epoch 14/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - accuracy: 0.9174 - loss: 0.2377\n",
      "Epoch 15/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9449 - loss: 0.1843\n",
      "Epoch 16/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9462 - loss: 0.1655\n",
      "Epoch 17/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.9615 - loss: 0.1396\n",
      "Epoch 18/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9614 - loss: 0.1198\n",
      "Epoch 19/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9692 - loss: 0.1083\n",
      "Epoch 20/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9686 - loss: 0.1119\n",
      "Epoch 21/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9815 - loss: 0.0733\n",
      "Epoch 22/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9894 - loss: 0.0521\n",
      "Epoch 23/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.9857 - loss: 0.0542\n",
      "Epoch 24/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9890 - loss: 0.0423\n",
      "Epoch 25/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9954 - loss: 0.0298\n",
      "Epoch 26/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9937 - loss: 0.0316\n",
      "Epoch 27/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.9971 - loss: 0.0226\n",
      "Epoch 28/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9960 - loss: 0.0230\n",
      "Epoch 29/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.9961 - loss: 0.0182\n",
      "Epoch 30/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9985 - loss: 0.0129\n",
      "Epoch 31/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.9954 - loss: 0.0216\n",
      "Epoch 32/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9945 - loss: 0.0164\n",
      "Epoch 33/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9979 - loss: 0.0140\n",
      "Epoch 34/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9975 - loss: 0.0106\n",
      "Epoch 35/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9977 - loss: 0.0109\n",
      "Epoch 36/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9985 - loss: 0.0088\n",
      "Epoch 37/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9969 - loss: 0.0091\n",
      "Epoch 38/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9999 - loss: 0.0038\n",
      "Epoch 39/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9988 - loss: 0.0053\n",
      "Epoch 40/40\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - accuracy: 0.9982 - loss: 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# モデルを学習する関数\n",
    "def train(X, y, X_test, y_test):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=X.shape[1:]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    # 修正: lr を learning_rate に変更し、 decay を削除\n",
    "    opt = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.fit(X, y, batch_size=28, epochs=40)\n",
    "\n",
    "    return model\n",
    "\n",
    "# メイン関数、データの読み込みとモデルの学習を行います。\n",
    "def main():\n",
    "    X_train, y_train, X_test, y_test = load_data()\n",
    "    model = train(X_train, y_train, X_test, y_test)\n",
    "    model.save(\"cnn.h5\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aa242c3-999e-44ca-a5b5-f986a1a37080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "[[0.99670213 0.00329782]]\n",
      ">>> 犬\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "\n",
    "imsize = (64, 64)\n",
    "\n",
    "testpic = \"img_snoopy.png\"\n",
    "keras_param = \"cnn.h5\"\n",
    "\n",
    "def load_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.convert(\"RGB\")  # RGBに変換\n",
    "    img = img.resize(imsize)\n",
    "    img = np.asarray(img)\n",
    "    img = img / 255.0\n",
    "    return img \n",
    "\n",
    "model = load_model(keras_param)\n",
    "img = load_image(testpic)\n",
    "prd = model.predict(np.array([img]))\n",
    "print(prd)\n",
    "\n",
    "prelabel = np.argmax(prd, axis=1)\n",
    "if prelabel == 0:\n",
    "    print(\">>> 犬\")\n",
    "elif prelabel == 1:\n",
    "    print(\">>> 猫\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
